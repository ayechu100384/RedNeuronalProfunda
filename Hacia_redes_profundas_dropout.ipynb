{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Redes neuronales profundas\n",
    "\n",
    "El objetivo de este proyecto es entrenar una red neuronal que sea capaz de clasificar imágenes en 10 clases correspondientes a: ‘frog’, ‘truck’, ‘deer’, automobile’, ‘bird’, ‘horse’, ‘ship’, ‘cat’, ‘dog’ and ‘airplane’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los datos de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga los siguientes datasets de train y test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load('10classXY/10_class_X.npy')\n",
    "Y = np.load('10classXY/10_class_Y.npy')\n",
    "Y = Y.T\n",
    "X /= 255\n",
    "######## Escoge el número de ejemplos de train. Si tu PC es muy lento, no elijas muchos\n",
    "num_ejemplos_train = 45000\n",
    "num_ejemplos_test = 50000 - num_ejemplos_train\n",
    "X_train = np.zeros((3072, num_ejemplos_train))\n",
    "Y_train = np.zeros((10, num_ejemplos_train))\n",
    "eleccion = np.random.choice(range(50000), num_ejemplos_train, False)\n",
    "no_eleccion = [x for x in range(50000) if x not in eleccion]\n",
    "for i in range(num_ejemplos_train):\n",
    "    X_train[:, i] = X[eleccion[i], :, :, :].ravel()\n",
    "    Y_train[:, i] = Y[:, eleccion[i]]\n",
    "X_test = np.zeros((3072, num_ejemplos_test))\n",
    "Y_test = np.zeros((10, num_ejemplos_test))\n",
    "for i in range(num_ejemplos_test):\n",
    "    X_test[:, i] = X[no_eleccion[i], :, :, :].ravel()\n",
    "    Y_test[:, i] = Y[:, no_eleccion[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Hay {} muestras de train').format(X_train.shape[1])\n",
    "print('Hay {} muestras de test').format(Y_test.shape[1])\n",
    "print('Cada muestra es de dimensión {}').format(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos las funciones Relu y Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    s = np.maximum(0,z)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    t = np.exp(z)\n",
    "    s = t / np.sum(t,axis=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parámetros iniciales: inicialización aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parametros_iniciales(info_capas):\n",
    "    parametros  = {}\n",
    "    L = len(info_capas)\n",
    "    for i in range(1,L-1):\n",
    "        #parametros['W' + str(i)] = np.random.randn(info_capas[i],info_capas[i-1]) * 0.01 #inicialización inicial\n",
    "        parametros['W' + str(i)] = np.random.randn(info_capas[i],info_capas[i-1]) * (2.0 / info_capas[i-1]) # inicialización RELU\n",
    "        parametros['b' + str(i)] = np.zeros((info_capas[i],1))\n",
    "    parametros['W' + str(L-1)] = np.random.randn(info_capas[L-1],info_capas[L-2]) * (1.0 / info_capas[L-2]) # inicialización SOFTMAX\n",
    "    parametros['b' + str(L-1)] = np.zeros((info_capas[L-1],1))\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para la propagación hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capa_adelante_relu(A_prev, W, b, prob_perm):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = relu(Z)\n",
    "    D = np.random.rand(A.shape[0], A.shape[1])\n",
    "    D = np.float32(D <= prob_perm)\n",
    "    A = A * D / prob_perm\n",
    "    return A, Z, D\n",
    "\n",
    "def capa_adelante_softmax(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = softmax(Z)\n",
    "    return A, Z # En la última capa no se hace dropout, no tiene sentido apagar las neuronas de la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagacion_adelante(X, parametros, prob_perm):\n",
    "    L = len(parametros) / 2\n",
    "    cache = {}\n",
    "    cache['A0'] = X # guardamos los parámetros de entrada\n",
    "    for i in range(1,L): # Para cada capa aplicamos la función relu\n",
    "        # A, Z = capa_adelante(A_prev, W, b, prob_perm)\n",
    "        cache['A' + str(i)], cache['Z' + str(i)], cache['D' + str(i)] = capa_adelante_relu(cache['A' + str(i-1)], parametros['W' + str(i)], parametros['b' + str(i)], prob_perm)\n",
    "    # en la ultima capa aplicamos la funcion softmax, para calcular la salida\n",
    "    cache['A' + str(L)], cache['Z' + str(L)] = capa_adelante_softmax(cache['A' + str(L-1)], parametros['W' + str(L)], parametros['b' + str(L)])\n",
    "    return cache['A' + str(L)], cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos ahora la función coste. Esta función recibe $A^{[L]}$ e $Y$ y calcula:\n",
    "$$J = -\\left[\\sum_{l=1}^{nc} Y \\log(A^{[l]})\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coste(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    J = np.multiply(Y, np.log(A))\n",
    "    J = np.sum(J,axis=0)\n",
    "    return -np.sum(J)/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para la propagación hacia atras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capa_atras_softmax(A, Y, Aprev, W):\n",
    "    dZ = A - Y\n",
    "    dW = np.dot(dZ, Aprev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def capa_atras_relu(dA, Z, Aprev, W, D): \n",
    "    dA = dA * D / prob_perm\n",
    "    dZ = dA * np.float32(Z>=0)\n",
    "    dW = np.dot(dZ, Aprev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagacion_atras(A, Y, cache, parametros):\n",
    "    m = Y.shape[1]\n",
    "    gradientes = {}\n",
    "    L = len(parametros) / 2\n",
    "    # primero calculamos la derivada respecto de A a partir de la salida\n",
    "    gradientes['dA'+str(L)] = - np.divide(Y,A) + np.divide(1-Y, 1-A)\n",
    "    # la última capa la calculamos fuera porque emplea la función sigmoide\n",
    "    # falta dividir entre m los dW y los db\n",
    "    dA_prev, dW, db = capa_atras_softmax(A, Y, cache['A' + str(L-1)], parametros['W' + str(L)])\n",
    "    gradientes['dA' + str(L-1)] = dA_prev\n",
    "    gradientes['dW' + str(L)] = dW / m\n",
    "    gradientes['db' + str(L)] = db / m\n",
    "    for l in range(L-1, 0, -1):\n",
    "        dA_prev, dW, db = capa_atras_relu(gradientes['dA' + str(l)], cache['Z' + str(l)], cache['A' + str(l-1)], parametros['W' + str(l)], cache['D' + str(l)])\n",
    "        gradientes['dA' + str(l-1)] = dA_prev\n",
    "        gradientes['dW' + str(l)] = dW / m\n",
    "        gradientes['db' + str(l)] = db / m\n",
    "    return gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función actualiza parámetros es muy parecida a las anteriores, pero ahora para L capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actualiza_parametros(parametros, gradientes, learning_rate):\n",
    "    L = len(parametros) / 2\n",
    "    for l in range(1, L+1):\n",
    "        parametros['W'+str(l)] = parametros['W'+str(l)] - learning_rate * gradientes['dW'+str(l)]\n",
    "        parametros['b'+str(l)] = parametros['b'+str(l)] - learning_rate * gradientes['db'+str(l)]\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Por último, vamos a definir la función que implementa el modelo completo. Recuerda guardar en costes el coste del sistema cada 'x' iteraciones (si no almacenaremos demasiados valores y la visualización no será tan clara)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def red_profunda(X, Y, X_test, Y_test, info_capas, learning_rate, num_iteraciones, batch_size, prob_perm):\n",
    "    m = Y.shape[1]\n",
    "    perm = np.random.permutation(m) # array random para coger lotes con ejemplos mezclados\n",
    "    parametros = parametros_iniciales(info_capas)\n",
    "    costes = []\n",
    "    tiempo_inicial = time() \n",
    "    for i in range(num_iteraciones):\n",
    "        for b in range(0, m, batch_size): # recorremos los lotes\n",
    "            if b+batch_size <= m:\n",
    "                X_batch = X[:,perm[b:b+batch_size]]\n",
    "                Y_batch = Y[:,perm[b:b+batch_size]]\n",
    "            else:# El último lote contendrá los ejemplos que falten (será más pequeño)\n",
    "                X_batch = X[:,perm[b:m]]\n",
    "                Y_batch = Y[:,perm[b:m]]\n",
    "            A, cache = propagacion_adelante(X_batch, parametros, prob_perm)\n",
    "            gradientes = propagacion_atras(A, Y_batch, cache, parametros)\n",
    "            parametros = actualiza_parametros(parametros, gradientes, learning_rate)\n",
    "        if i % 200 == 0:\n",
    "            coste_aux = coste(A, Y_batch)\n",
    "            costes.append(coste_aux)\n",
    "            tiempo_final = time()\n",
    "            tiempo_ejecucion = tiempo_final - tiempo_inicial\n",
    "            ptrain = predecir(X, Y, parametros, prob_perm=1)\n",
    "            ptest = predecir(X_test, Y_test, parametros, prob_perm=1)\n",
    "            print 'Iteracion',i,'coste',coste_aux,'%train',ptrain*100,'p%test',ptest*100,'Tiempo de ejecución',tiempo_ejecucion\n",
    "    tiempo_final = time()\n",
    "    tiempo_ejecucion = tiempo_final - tiempo_inicial\n",
    "    print 'Iteracion',i,'coste',coste_aux,'%train',ptrain*100,'p%test',ptest*100,'Tiempo de ejecución',tiempo_ejecucion\n",
    "    plt.plot(costes)\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función predecir devuelve el porcentaje de aciertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predecir(X, Y, parametros, prob_perm):\n",
    "    m = Y.shape[1]\n",
    "    A, cache = propagacion_adelante(X, parametros,prob_perm=1)\n",
    "    A_max = np.argmax(A, axis=0)\n",
    "    Y_max = np.argmax(Y, axis=0)\n",
    "    return np.sum(A_max == Y_max,axis=0) /float(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables a fijar por el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx = X_train.shape[0] # FIJO número de características/dimensiones\n",
    "print X_train.shape[1]\n",
    "print X_test.shape[1]\n",
    "nh1 = 20 # número de neuronas de la capa oculta 1\n",
    "nh2 = 15 # número de neuronas de la capa oculta 2\n",
    "nh3 = 15 # número de neuronas de la capa oculta 3\n",
    "nh4 = 15 # número de neuronas de la capa oculta 4\n",
    "ny = 10 # FIJO número de neuronas de la capa de salida = número de clases\n",
    "info_capas = [nx, nh1, nh2, nh3,  ny]\n",
    "learning_rate = 0.05\n",
    "num_iteraciones = 10000\n",
    "batch_size = 512 # Tamaño de los lotes (anular batches batch_size = m)\n",
    "prob_perm = 1 # Probabilidad de permanencia (anular drop-out prob_perm = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJECUCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parametros1 = red_profunda(X_train, Y_train, X_test, Y_test, info_capas, learning_rate, num_iteraciones, batch_size, prob_perm) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
